<!DOCTYPE html>
<html>
<title>Justin Le</title>
<meta name="author" content="Justin Le, UNLV">
<link rel="stylesheet" type="text/css" href="main.css">
<link rel="shortcut icon" href="./main.ico">
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-80875071-1', 'auto');
  ga('send', 'pageview');
</script>

<body>
<div class="course">
<table>
<td>
    <p class="hi">A Technical Introduction to <br> Machine Learning</p>
    <p>Hosted by <a href="http://justin-le.github.io/">Justin Le</a></p>
    <br>
    <p>Friday, Oct. 7, 2016</p> <p> 2 to 4pm</p> <p>SEB 3265, UNLV</p>
    <br>
    <p><b>Abstract</b></p>
    <p>In recent years, intelligent algorithms that learn from data have had an enormous impact on empirical research in such diverse areas as medicine, physics, finance, and beyond. Furthermore, these algorithms have been implemented in common programming languages, making them widely accessible to both researchers and developers.</p>
    <p>In this workshop, we'll discuss the concepts and mathematics that underlie these machine learning techniques, as well as the Python libraries that enable us to efficiently apply them in practice. Requiring only a basic familiarity with calculus and programming, the workshop will introduce common challenges in designing, characterizing, and applying machine learning methods with the goal of extracting insights from data.</p>
    <br>
    <p><b>Schedule</b></p>
    <p>Introduction</p>
    <ul>
        <li>Making tradeoffs between models</li>
        <li>A framework for applying machine learning</li>
        <li>What's important in theory and in practice?</li>
    </ul>
    <p>Building flexible models and managing their complexity</p>
    <ul>
        <li>Classification: logistic regression, support vector machines, and decision trees</li>
        <li>Overfitting, robustness, and generalizable models</li>
        <li>Ensembles: random forests and gradient-boosted machines</li>
    </ul>
    <p>Performance metrics and model evaluation</p>
    <ul>
        <li>Precision, recall, and the ROC curve</li> 
        <li>Cross-validation</li> 
        <li>Generalization error and data snooping</li>
    </ul>
    <p>Features and representation</p>
    <ul>
        <li>Visually exploring data</li>
        <li>Constructing useful features</li>
        <li>How features impact classification</li>
        <li>Deep learning: CNNs, long short-term memory, and the frontiers of hard intelligence</li>
    </ul>
    <p>Applications to real-world data</p>
    <ul>
        <li>Mining text data: the 2016 U.S. Presidential Debate</li>
        <li>Predicting customer satisfaction: Santander Bank</li>
        <li>Detecting cancer cells</li>
    </ul>
    <p>Additional topics (as time permits)</p>
    <ul>
        <li>Preparing data: standardizing, normalizing, and imputation</li> 
        <li>An introduction to optimization: gradient methods and convergence analysis</li>
    </ul>
    <br>
    <p><b>Prerequisites</b></p>
    <p>Although not necessary, it is strongly recommended that you review the following topics before attending in order to fully benefit from the workshop:</p>
    <ul>
        <li>Mathematics: vector/matrix calculations, probability distributions, sums/integrals, gradients</li>
        <li>Programming: basic Linux commands, basic Python commands involving lists and functions</li>
    </ul>
    <br>
    <p><b>Notes</b></p>
    <p>If you wish to follow along with our programming exercises during the workshop, please bring your own laptop with Ubuntu 14.04+ and the following packages:</p>
    <pre><code>
    sudo apt-get install build-essential python-dev python-numpy \
      python-numpy-dev python-scipy libatlas-dev g++ python-matplotlib \
      ipython ipython-notebook
    </pre></code>
    <p>Also, please install <a href="http://xgboost.readthedocs.io/en/latest/build.html">xgboost for Python</a> if you wish to run the examples for gradient boosting.</p>
    <p>If you use your own distro instead, we cannot offer any support if you encounter issues while executing our code.</p>
</td>
</table>
</div>
</body>
</html> 
